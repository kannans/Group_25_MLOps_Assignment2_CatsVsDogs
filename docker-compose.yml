version: '3.8'

services:
  inference-service:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - LOG_LEVEL=INFO
    restart: always

  mlflow-server:
    image: ghcr.io/mlflow/mlflow
    ports:
      - "5001:5000"
    command: mlflow ui --host 0.0.0.0
